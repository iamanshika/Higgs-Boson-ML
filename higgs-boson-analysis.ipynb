{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,normalize,MinMaxScaler,Normalizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "%matplotlib inline\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from IPython.display import display, HTML\n",
    "from dtreeviz.trees import dtreeviz\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining two functions that are used to get a report of our datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(df):\n",
    "    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )\n",
    "def tbl_report(tbl, cols=None, card=10):\n",
    "    print(\"Table Shape\", tbl.shape)\n",
    "    dtypes = tbl.dtypes\n",
    "    nulls = []\n",
    "    uniques = []\n",
    "    numuniques = []\n",
    "    vcs = []\n",
    "    for col in dtypes.index:\n",
    "        n = tbl[col].isnull().sum()\n",
    "        nulls.append(n)\n",
    "        strdtcol = str(dtypes[col])\n",
    "        #if strdtcol == 'object' or strdtcol[0:3] == 'int' or strdtcol[0:3] == 'int':\n",
    "        #print(strdtcol)\n",
    "        uniqs = tbl[col].unique()\n",
    "        uniquenums = uniqs.shape[0]\n",
    "        if uniquenums < card: # low cardinality\n",
    "            valcounts = pd.value_counts(tbl[col], dropna=False)\n",
    "            vc = \"\\n\".join([\"{}:{}\".format(k,v) for k, v in valcounts.items()])\n",
    "        else:\n",
    "            vc='HC' # high cardinality\n",
    "        uniques.append(uniqs)\n",
    "        numuniques.append(uniquenums)\n",
    "        vcs.append(vc)\n",
    "    nullseries = pd.Series(nulls, index=dtypes.index)\n",
    "    uniqueseries = pd.Series(uniques, index=dtypes.index)\n",
    "    numuniqueseries = pd.Series(numuniques, index=dtypes.index)\n",
    "    vcseries = pd.Series(vcs, index=dtypes.index)\n",
    "    df = pd.concat([dtypes, nullseries, uniqueseries, numuniqueseries, vcseries], axis=1)\n",
    "    df.columns = ['dtype', 'nulls', 'uniques', 'num_uniques', 'value_counts']\n",
    "    if cols:\n",
    "        return pretty_print(df[cols])\n",
    "    return pretty_print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading and Reading Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading train set and loading test set\n",
    "train = pd.read_csv(\"/kaggle/input/higgs-boson/training.zip\")\n",
    "test = pd.read_csv(\"/kaggle/input/higgs-boson/test.zip\")\n",
    "\n",
    "#EventID is identifier - making it an index in both the sets\n",
    "train.set_index('EventId',inplace = True)\n",
    "test.set_index('EventId',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at top 5 rows in train\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at training set info \n",
    "tbl_report(train, cols=['dtype', 'nulls', 'num_uniques', 'value_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the numerical descriptions\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at top 5 rows in test\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at test info\n",
    "tbl_report(test, cols=['dtype', 'nulls', 'num_uniques', 'value_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at statistical description of test\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into X and y\n",
    "X = train.drop(['Weight','Label'],axis=1)\n",
    "y = pd.factorize(train['Label'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us look at the Class Ratio in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the count of each class in our label\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.countplot(train['Label']);\n",
    "for p in ax.patches:\n",
    "        ax.annotate('{:d}'.format(p.get_height()), (p.get_x()+0.3, p.get_height()+5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we do indeed have an imbalanced dataset. Considering this, accuracy would not be a good metric of performance. F1 score would be a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding distribution of each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Distribution of each feature\n",
    "fig=plt.figure(figsize=(30,40))\n",
    "\n",
    "for i in range(np.shape(train)[1]-1):\n",
    "    ax = fig.add_subplot(8,4,i+1)\n",
    "    ax = sns.distplot(train.iloc[:,i], color = 'dodgerblue')\n",
    "    ax.set_title(\"Feature \"+ train.columns[i] +\" distribution\")\n",
    "    ax.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Univariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the distribution of Data Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Distribution of features per class\n",
    "fig=plt.figure(figsize=(30,40))\n",
    "\n",
    "for i in range(np.shape(train)[1]-2):\n",
    "    ax = fig.add_subplot(6,5,i+1)\n",
    "    ax = sns.distplot(train[train['Label'] == 's'].iloc[:,i],label=\"Class S\", color = \"blue\")\n",
    "    ax = sns.distplot(train[train['Label'] == 'b'].iloc[:,i],label=\"Class B\", color = \"grey\")\n",
    "    ax.set_title(\"Feature \"+ train.columns[i] +\" distribution per class\")\n",
    "    ax.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding highly correlated Features - and printing the pairs of highly correlated features with threshold of 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr().style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the highly correlated features along with their correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to store the features with high correlation as tuples\n",
    "feat=[]\n",
    "#Setting a threshold of 0.9 of correlation\n",
    "threshold=0.9\n",
    "correlation=X.corr()\n",
    "for i in X.columns:\n",
    "    temp=correlation[i]\n",
    "    #Finding the correlated features greater than the threshold\n",
    "    corr_features=temp[(abs(temp)>threshold) & (temp.index!=i)].index.values\n",
    "    #Adding the correlated features into a list keeping in mind that there is only one occurrence of the feature combination\n",
    "    if(len(corr_features)!=0):\n",
    "        for j in corr_features:\n",
    "            features=(i,j)\n",
    "        \n",
    "            if(len(feat)==0):\n",
    "                feat.append(features)\n",
    "            else:\n",
    "                count=len(feat)\n",
    "                for x in feat:\n",
    "                    if set(x) != set(features):\n",
    "                        count-=1  \n",
    "                    else:\n",
    "                        break\n",
    "                if(count==0):\n",
    "                    feat.append(features)\n",
    "                #[feat.append(features) for x in feat if not (set(x)==set(features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The highly correlated features are given below\")\n",
    "for i in feat:\n",
    "    corr=correlation[i[0]][i[1]]\n",
    "    print('Features '+i[0]+' and '+i[1]+' are correlated with a correlation index of '+ str(np.round(corr,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Getting the Data Ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have an imbalanced dataset, we are using SMOTE for upsampling and downsampling our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Dropping Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "for i, j in feat:\n",
    "    if i in count.keys():\n",
    "        count[i]+= 1\n",
    "    else:\n",
    "        count[i] = 1\n",
    "    if j in count.keys():\n",
    "        count[j]+= 1\n",
    "    else:\n",
    "        count[j] = 1\n",
    "for k, v in count.items():\n",
    "    if v > 2:\n",
    "        X.drop(k, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) SMOTE Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(pd.DataFrame(y))\n",
    "print('Before', counter)\n",
    "\n",
    "smt = SMOTE()\n",
    "\n",
    "#oversampling using SMOTE\n",
    "X_sm, y_sm = smt.fit_resample(X,y)\n",
    "\n",
    "counter = Counter(y_sm)\n",
    "print('After', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.concat([X_sm,pd.DataFrame(y_sm,columns=['Label'])],axis=1)\n",
    "\n",
    "f, axes = plt.subplots(figsize=(15, 10), dpi=100)\n",
    "plt.subplot(121)\n",
    "sns.despine()\n",
    "sns.scatterplot(x=temp_df['PRI_met_sumet'], y=temp_df['DER_pt_ratio_lep_tau'], hue = temp_df['Label'], data=temp_df)\n",
    "plt.title('Resampling with SMOTE', fontsize=14);\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.despine()\n",
    "sns.scatterplot(x=train['PRI_met_sumet'], y=train['DER_pt_ratio_lep_tau'], hue = train['Label'], data=train)\n",
    "plt.title('No resampling', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creation of a Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing Categorical and Continuous Variables\n",
    "cat_vars = ['PRI_jet_num']\n",
    "cont_vars = np.array(train.drop(['PRI_jet_num','Weight','Label'],axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions to make Model building and Cross-validation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        print(\"SCORE FUNC\", score_func)\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print(\"BEST\", gs.best_params_, gs.best_score_)\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\n",
    "def do_classify(clf, parameters, indf,y,score_func, n_folds=5, n_jobs=1):\n",
    "    \n",
    "    X = indf\n",
    "    y = y\n",
    "    \n",
    "    Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,train_size=0.8,random_state=214)\n",
    "    \n",
    "    clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
    "    \n",
    "    clf = clf.fit(Xtrain, ytrain)\n",
    "    \n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    \n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"############# Based On Standard Predict ################\")\n",
    "    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "    print()\n",
    "    print(confusion_matrix(ytest, clf.predict(Xtest)))\n",
    "    print(\"########################################################\")\n",
    "    print()\n",
    "    \n",
    "    plot_confusion_matrix(clf,Xtest,ytest,cmap=\"Blues\")\n",
    "    return clf, Xtrain, ytrain, Xtest,ytest\n",
    "\n",
    "def p_importance(model, cols, fi, fistd = 0):\n",
    "    return pd.DataFrame({'features':cols, 'importance':fi, 'importance_std': fistd}\n",
    "                       ).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pipeline for Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Normalization\n",
    "normalize = Normalizer()\n",
    "\n",
    "# Set up One Hot Encoding of Target Variable\n",
    "oh = OneHotEncoder()\n",
    "\n",
    "# Continuous Variables Neet to be normalized\n",
    "cont_pipe = Pipeline([('normalize', normalize)])\n",
    "\n",
    "# Categorical variables need to be one hot encoded\n",
    "cat_pipe = Pipeline([('onehot', oh)])\n",
    "\n",
    "# Combine both into a transformer\n",
    "transformers = [('cont', cont_pipe, cont_vars), ('cat', cat_pipe, cat_vars)]\n",
    "\n",
    "# Apply transformer to relevant columns. Nothing will be done for the rest\n",
    "ct = ColumnTransformer(transformers=transformers, remainder=\"passthrough\")\n",
    "\n",
    "# Create a pipeline so that we are not leaking data from validation to train in the individual folds\n",
    "pipe_lr = Pipeline(steps=[('ct', ct), ('model', LogisticRegression(max_iter=10000, penalty='l2'))])\n",
    "\n",
    "# In paramgrid we dont use C but use model__C corresponding to the name in the pipeline\n",
    "paramgrid_lr = dict(model__C=[1000, 100, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train our model. Our do_classify takes care of subsetting the data and pickinging up the target variable.We score using the AUC on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, X_train, y_train, X_test, y_test = do_classify(pipe_lr, paramgrid_lr, X_sm, y_sm, score_func='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pipeline for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline so that we are not leaking data from validation to train in the individual folds\n",
    "pipe_rf = Pipeline(steps=[('ct', ct), ('rf', RandomForestClassifier())])\n",
    "paramgrid_rf = { 'rf__max_features': ['auto', 'sqrt'],\n",
    "  'rf__max_depth': [5,10, 20, None], \n",
    "  'rf__min_samples_split': [2, 5, 10], \n",
    "  'rf__min_samples_leaf': [1, 2, 4],\n",
    "  'rf__bootstrap': [True, False]}\n",
    "\n",
    "rf, X_train, y_train, X_test, y_test = do_classify(pipe_rf, paramgrid_rf, X_sm, y_sm, score_func='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Pipeline for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline so that we are not leaking data from validation to train in the individual folds\n",
    "pipe_dt = Pipeline(steps=[('ct', ct), ('dt', DecisionTreeClassifier(random_state=142))])\n",
    "paramgrid_dt = {'dt__max_depth':range(1,9),'dt__min_samples_leaf':range(3,5),'dt__criterion':['gini']}\n",
    "\n",
    "\n",
    "clf, Xtrain, ytrain, Xtest,ytest = do_classify(pipe_dt, paramgrid_dt,X,y,'roc_auc',n_folds=5,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [None, None,['red','blue'],]\n",
    "dt_viz = dtreeviz(DecisionTreeClassifier(random_state=142), X,y,\n",
    "               feature_names = X.columns,\n",
    "               target_name = 'Label', class_names= ['S','B']\n",
    "              ,orientation = 'TD',\n",
    "               colors={'classes':colors},\n",
    "               label_fontsize=12,\n",
    "               ticks_fontsize=10,\n",
    "               )\n",
    "\n",
    "dt_viz.save(\"DecisionTreeClassifier.svg\")\n",
    "\n",
    "dt_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimp = permutation_importance(DecisionTreeClassifier(random_state=142),Xtest,ytest)\n",
    "ddf = p_importance(DecisionTreeClassifier(random_state=142),list(X.columns),dimp['importances_mean'],dimp['importances_std']).iloc[:10]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(17,10))\n",
    "sns.barplot(data=ddf,x='features',y='importance',label='Decision_importances',ax=ax)\n",
    "plt.xticks(rotation='45')\n",
    "plt.title(\"Bar plot of Importances for Decision Tree Model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X , y ,train_size=0.8)\n",
    "\n",
    "dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n",
    "dvalid = xgb.DMatrix(Xtest, label=ytest)\n",
    "\n",
    "xgb_pars = {'min_child_weight': 100, \n",
    "            'eta': 0.04, \n",
    "            'colsample_bytree': 0.8, \n",
    "            'max_depth': 100,\n",
    "            'subsample': 0.75, \n",
    "            'lambda': 2, \n",
    "            'nthread': -1, \n",
    "            'booster' : \n",
    "            'gbtree', \n",
    "            'silent': 1, \n",
    "            'gamma' : 0,\n",
    "            'eval_metric': 'mae', \n",
    "            'objective': 'reg:linear'}    \n",
    "\n",
    "boost = xgb.train(xgb_pars, dtrain, 500, maximize=False, verbose_eval=15) \n",
    "\n",
    "y_pred = clf3.predict(dvalid)\n",
    "y_pred = [1 if y>0.5 else 0 for y in y_pred]\n",
    "\n",
    "print(classification_report(ytest,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plotting and Comparing ROC Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5,  proba=True, skip=0, initial = False):\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    if proba:\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', lw=2, alpha=0.4, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', lw=2, alpha=0.4, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    for k in range(0, fpr.shape[0],labe):\n",
    "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "        threshold = str(np.round(thresholds[k], 2))\n",
    "        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_roc('logistic', lr, ytest ,xtest, ax=ax, labe=10,  proba=True, skip=16300, initial = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_roc('randomForest', rf,ytest ,Xtest, ax=ax, labe=10,  proba=True, skip=16300, initial = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_roc('decisionTree', dt,ytest ,Xtest, ax=ax, labe=10,  proba=True, skip=16300, initial = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_roc('xgbboost', boost,ytest ,Xtest, ax=ax, labe=10,  proba=True, skip=16300, initial = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Self-supervised learning in Tabular Domain using VIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Initial hyperparameters\n",
    "# Experimental parameters\n",
    "label_no = 1000  \n",
    "model_sets = ['logit','xgboost','mlp']\n",
    "  \n",
    "# Hyper-parameters\n",
    "p_m = 0.3\n",
    "alpha = 0.01\n",
    "K = 4\n",
    "beta = 1.0\n",
    "label_data_rate = 0.6\n",
    "\n",
    "# Metric\n",
    "metric = 'acc'\n",
    "  \n",
    "# Define output\n",
    "results = np.zeros([len(model_sets)+2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling data\n",
    "scaler = StandardScaler()\n",
    "X_sm = scaler.fit_transform(X_sm)\n",
    "\n",
    "#Encoding Data\n",
    "y_sm = np.where(y_sm == 'b',1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into unlabelled data and defining Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting X and y into train and test \n",
    "x_train_, x_test_, y_train_, y_test_ = train_test_split(X_sm, y_sm, train_size=0.7, random_state=123)\n",
    "\n",
    "#Spillting into unlabelled and labelled\n",
    "idx = np.random.permutation(len(y_train))\n",
    "\n",
    "# Label data : Unlabeled data = label_data_rate:(1-label_data_rate)\n",
    "label_idx = idx[:int(len(idx)*label_data_rate)]\n",
    "unlab_idx = idx[int(len(idx)*label_data_rate):]\n",
    "\n",
    "# Unlabeled data\n",
    "x_unlab = x_train.iloc[unlab_idx, :]\n",
    "\n",
    "# Labeled data\n",
    "x_train = x_train.iloc[label_idx, :]  \n",
    "y_train = y_train[label_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Mask Generator\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask Generator\n",
    "def mask_generator (p_m, x):\n",
    "    mask = np.random.binomial(1, p_m, x.shape)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Pre-text Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-text Generator\n",
    "def pretext_generator (m, x):  \n",
    "    \"\"\"Generate corrupted samples.\n",
    "\n",
    "    Args:\n",
    "    m: mask matrix\n",
    "    x: feature matrix\n",
    "\n",
    "    Returns:\n",
    "    m_new: final mask matrix after corruption\n",
    "    x_tilde: corrupted feature matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    no, dim = x.shape  \n",
    "    \n",
    "    # Randomly (and column-wise) shuffle data\n",
    "    x_bar = np.zeros([no, dim])\n",
    "    \n",
    "    for i in range(dim):\n",
    "        idx = np.random.permutation(no)\n",
    "        x_bar[:, i] = x.iloc[idx, i]\n",
    "\n",
    "    # Corrupt samples\n",
    "    x_tilde = x * (1-m) + x_bar * m  \n",
    "    \n",
    "    # Define new mask matrix\n",
    "    m_new = 1 * (x != x_tilde)\n",
    "\n",
    "    return m_new, x_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "def mlp(x_train, y_train, x_test, parameters): \n",
    "\n",
    "    # Convert labels into proper format\n",
    "    if len(y_train.shape) == 1:\n",
    "        no = len(y_train)\n",
    "        dim = len(np.unique(y_train))\n",
    "\n",
    "        # Define output\n",
    "        matrix = np.zeros([no,dim])\n",
    "\n",
    "        # Convert vector to matrix\n",
    "        for i in range(dim):\n",
    "            idx = np.where(y_train == i)\n",
    "            matrix[idx, i] = 1\n",
    "\n",
    "        y_train = matrix\n",
    "\n",
    "    # Divide training and validation sets (9:1)\n",
    "    idx = np.random.permutation(len(x_train[:, 0]))\n",
    "    train_idx = idx[:int(len(idx)*0.9)]\n",
    "    valid_idx = idx[int(len(idx)*0.9):]\n",
    "\n",
    "    # Validation set\n",
    "    x_valid = x_train[valid_idx, :]\n",
    "    y_valid = y_train[valid_idx, :]\n",
    "\n",
    "    # Training set\n",
    "    x_train = x_train[train_idx, :]\n",
    "    y_train = y_train[train_idx, :]  \n",
    "\n",
    "    # Define network parameters\n",
    "    hidden_dim = parameters['hidden_dim']\n",
    "    epochs_size = parameters['epochs']\n",
    "    act_fn = parameters['activation']\n",
    "    batch_size = parameters['batch_size']\n",
    "\n",
    "    # Define basic parameters\n",
    "    data_dim = len(x_train[0, :])\n",
    "    label_dim = len(y_train[0, :])\n",
    "\n",
    "    # Build model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_dim, input_dim = data_dim, activation = act_fn))\n",
    "    model.add(Dense(hidden_dim, activation = act_fn))\n",
    "    model.add(Dense(hidden_dim, activation = act_fn))\n",
    "    model.add(Dense(hidden_dim, activation = act_fn))\n",
    "    model.add(Dense(label_dim, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='adam', \n",
    "                metrics = ['acc'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode = 'min', \n",
    "                     verbose = 1, restore_best_weights=True, patience=50)\n",
    "\n",
    "    # Fit model on training dataset\n",
    "    history = model.fit(x_train, y_train, validation_data = (x_valid, y_valid), \n",
    "            epochs = epochs_size, batch_size = batch_size, \n",
    "            verbose = 0, callbacks=[es])\n",
    "\n",
    "    # Predict on x_test\n",
    "    y_test_hat = model.predict(x_test)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    return y_test_hat, history, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "\n",
    "# Parameters\n",
    "_, dim = x_unlab.shape\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "# Build model  \n",
    "inputs = Input(shape=(dim,))\n",
    "\n",
    "# Encoder  \n",
    "h = Dense(int(dim), activation='relu')(inputs)  \n",
    "\n",
    "# Mask estimator\n",
    "output_1 = Dense(dim, activation='sigmoid', name = 'mask')(h)  \n",
    "\n",
    "# Feature estimator\n",
    "output_2 = Dense(dim, activation='sigmoid', name = 'feature')(h)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = [output_1, output_2])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss={'mask': 'binary_crossentropy', \n",
    "                  'feature': 'mean_squared_error'},\n",
    "            loss_weights={'mask':1, 'feature':alpha})\n",
    "\n",
    "# Generate corrupted samples\n",
    "m_unlab = mask_generator(p_m, x_unlab)\n",
    "m_label, x_tilde = pretext_generator(m_unlab, x_unlab)\n",
    "\n",
    "# Fit model on unlabeled data\n",
    "encoder_history = model.fit(x_tilde, {'mask': m_label, 'feature': x_unlab}, \n",
    "        epochs = epochs, batch_size= batch_size)\n",
    "\n",
    "# Extract encoder part\n",
    "layer_name = model.layers[1].name\n",
    "layer_output = model.get_layer(layer_name).output\n",
    "encoder = models.Model(inputs=model.input, outputs=layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,7))\n",
    "# plot loss\n",
    "plt.title('Mask Loss')\n",
    "plt.plot(encoder_history.history['mask_loss'], color='orange', label='Mask Loss')\n",
    "plt.plot(encoder_history.history['feature_loss'], color='red', label='Feature Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_parameters = dict()\n",
    "mlp_parameters['hidden_dim'] = 256\n",
    "mlp_parameters['epochs'] = 1000\n",
    "mlp_parameters['activation'] = 'sigmoid'\n",
    "mlp_parameters['batch_size'] = 128\n",
    "\n",
    "x_train_hat = encoder.predict(x_train)\n",
    "x_test_hat = encoder.predict(x_test)\n",
    "      \n",
    "y_test_hat, MLP_history, acc = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "\n",
    "print('VIME-Self Accuracy: ' + str(np.round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,7))\n",
    "# plot loss\n",
    "plt.subplot(121)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(MLP_history.history['loss'], color='blue', label='train')\n",
    "plt.plot(MLP_history.history['val_loss'], color='orange', label='test')\n",
    "plt.legend()\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(122)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(MLP_history.history['acc'], color='blue', label='train')\n",
    "plt.plot(MLP_history.history['val_acc'], color='orange', label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
